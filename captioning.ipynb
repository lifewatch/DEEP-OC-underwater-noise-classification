{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93335c72-e763-41ab-8784-2953839f52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_10875/2082739506.py:46: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(backend='soundfile')\n",
      "/srv/CLAP/dataset.py:36: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(backend='soundfile')\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as torchmodels\n",
    "from torch import nn\n",
    "import utils as u\n",
    "\n",
    "from filterbank import STFT, MelFilter, Log1p, MedFilt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, ClapModel, ClapAudioModelWithProjection, ClapProcessor\n",
    "import torch.nn.functional as F\n",
    "torch.hub.set_dir('/data/woutdecrop/torch/')\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# import fairseq\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import scipy\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "# from maad import util\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import suntime\n",
    "import pytz\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "\n",
    "import models\n",
    "import utils as u\n",
    "\n",
    "torchaudio.set_audio_backend(backend='soundfile')\n",
    "\n",
    "# matplotlib.use('TkAgg')\n",
    "# Get the color map by name:\n",
    "cm = plt.get_cmap('jet')\n",
    "\n",
    "from dataset import LifeWatchDataset\n",
    "import yaml\n",
    "import pathlib\n",
    "import numpy as np\n",
    "config_path=r\"config.yaml\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1408719c-2142-4398-ad21-edb697cf1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08_12-30_a5_b0.5_weights_0.1_0.2_0.3_0.4_\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Example directory path\n",
    "result_dir = '/srv/CLAP/3_1_Results/2024-08-08_12-30_a5_b0.5_weights_0.1_0.2_0.3_0.4_'\n",
    "\n",
    "# Extract folder name\n",
    "folder_name = os.path.basename(result_dir.rstrip('/'))\n",
    "print(folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73162b3d-acc3-4ce4-9573-253f00a5a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected CUDA device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "stop_shuffle=False\n",
    "sample_dur=10\n",
    "desired_fs = 48000\n",
    "wavs_folder= '/storage/Imagine_UC6_new/DATA/data_per_station_4_updated_metadata_extra_filter-window-4'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Selected CUDA device:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "# device =torch.device('cuda:0')\n",
    "# log_path = 'roi/BioLingual/logs_test.log'\n",
    "# log_file = open(log_path, mode='w')\n",
    "\n",
    "# detections = convert_raven_to_ae_format(labels_to_exclude=None)\n",
    "# detections = detections.loc[~detections.label.isna()]\n",
    "\n",
    "d_train_path= \"ds_split-CLAP/train.txt\"\n",
    "d_test_path= \"ds_split-CLAP/test.txt\"\n",
    "# print(\"path \", d_train_path)\n",
    "split = np.genfromtxt(d_train_path, dtype='str', delimiter=' ')\n",
    "d_train_loc = np.array([os.path.join(wavs_folder, i) for i in split[:, 0]])\n",
    "\n",
    "split = np.genfromtxt(d_test_path, dtype='str', delimiter=' ')\n",
    "d_test_loc = np.array([os.path.join(wavs_folder, i) for i in split[:, 0]])\n",
    "\n",
    "# split = np.genfromtxt(d_train_path, dtype='str', delimiter=' ')\n",
    "# d_train_loc = np.array([os.path.join(wavs_folder, i) for i in split[:, 0]])\n",
    "\n",
    "# split = np.genfromtxt(d_valid_path, dtype='str', delimiter=' ')\n",
    "# d_valid_loc = np.array([os.path.join(wavs_folder, i) for i in split[:, 0]])\n",
    "d_train=u.process_filenames(d_train_loc)\n",
    "d_test=u.process_filenames(d_test_loc)\n",
    "\n",
    "train_labels = set(d_train[\"label\"])\n",
    "# Count the occurrences of each label in d_train\n",
    "label_counts_train = d_train[\"label\"].value_counts()\n",
    "\n",
    "valid_train_labels = label_counts_train[label_counts_train >= 5].index\n",
    "\n",
    "\n",
    "# # Filter d_train and d_valid based on valid_train_labels\n",
    "# d_train = d_train[d_train[\"label\"].isin(valid_train_labels)]\n",
    "# test_train_labels = label_counts_train[label_counts_train >= 5].index\n",
    "\n",
    "# d_test = d_test[d_test[\"label\"].isin(test_train_labels)]\n",
    "\n",
    "# # Display the number of unique classes\n",
    "# num_classes = len(valid_train_labels)\n",
    "# ids={lbl: i for i, lbl in enumerate(d_train['label'].unique())}\n",
    "\n",
    "# similarity_matrix=u.similarity(ids,device,distance_weight = 0.6, speed_weight = 0.30,activity_weight = 0.10)\n",
    "# similarity_matrix_distance=u.similarity(ids,device,distance_weight = 1)\n",
    "# similarity_matrix_speed=u.similarity(ids,device,speed_weight = 1)\n",
    "# similarity_matrix_activity=u.similarity(ids,device,activity_weight = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4ddff3-4bf2-4905-943f-b32e12baed71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cargo at distance 4-5 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 3-4 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 1-2 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 5-6 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 2-3 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 3-4 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 4-5 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 2-3 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 5-6 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 1-2 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 10+ km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 0-1 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 7-8 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 7-8 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 8-9 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 9-10 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 0-1 km with speed 8.2-13.3 is underway-using-engine',\n",
       "       'Cargo at distance 3-4 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 5-6 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 3-4 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 4-5 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 4-5 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 2-3 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 8-9 km with speed 0-1.5 is at-anchor',\n",
       "       'Cargo at distance 5-6 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 10+ km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 3-4 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 2-3 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 2-3 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 3-4 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 2-3 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 1-2 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 4-5 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 3-4 km with speed 13.3+ is moored',\n",
       "       'Cargo at distance 3-4 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 5-6 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 7-8 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 4-5 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 2-3 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 2-3 km with speed 13.3+ is moored',\n",
       "       'Cargo at distance 3-4 km with speed 8.2-13.3 is underway-sailing',\n",
       "       'Cargo at distance 1-2 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 5-6 km with speed 13.3+ is moored',\n",
       "       'Cargo at distance 4-5 km with speed 13.3+ is moored',\n",
       "       'Cargo at distance 1-2 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 7-8 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 4-5 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 5-6 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 4-5 km with speed 8.2-13.3 is underway-sailing',\n",
       "       'Cargo at distance 5-6 km with speed 8.2-13.3 is underway-sailing',\n",
       "       'Cargo at distance 8-9 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 5-6 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 2-3 km with speed 8.2-13.3 is underway-sailing',\n",
       "       'Cargo at distance 8-9 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 10+ km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 9-10 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 9-10 km with speed 0-1.5 is underway-using-engine',\n",
       "       'Cargo at distance 1-2 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 6-7 km with speed 8.2-13.3 is constrained-by-her-draught',\n",
       "       'Cargo at distance 6-7 km with speed 13.3+ is moored',\n",
       "       'Cargo at distance 8-9 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 1-2 km with speed 13.3+ is underway-using-engine',\n",
       "       'Cargo at distance 8-9 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 7-8 km with speed 0-1.5 is restricted-maneuverability',\n",
       "       'Cargo at distance 7-8 km with speed 1.5-8.2 is underway-using-engine',\n",
       "       'Cargo at distance 10+ km with speed 8.2-13.3 is moored',\n",
       "       'Cargo at distance 10+ km with speed 0-1.5 is underway-using-engine'],\n",
       "      dtype='object', name='label')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89497e98-a14f-4540-b5fb-af815bc304b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ClapModel' object has no attribute 'generate_caption'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m clap_model \u001b[38;5;241m=\u001b[39m ClapModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdavidrrobinson/BioLingual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Load audio files\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# audio_files = ['audio_file']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate captions for the recording\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m captions \u001b[38;5;241m=\u001b[39m \u001b[43mclap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_caption\u001b[49m(audio_files, resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, entry_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m67\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print the result\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(audio_files)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ClapModel' object has no attribute 'generate_caption'"
     ]
    }
   ],
   "source": [
    "\n",
    "# from msclap import CLAP\n",
    "\n",
    "# Load and initialize CLAP\n",
    "model_path='davidrrobinson/BioLingual'\n",
    "clap_model=ClapAudioModelWithProjection.from_pretrained(model_path)\n",
    "clap_model = ClapModel.from_pretrained(pretrained_model_name_or_path = 'davidrrobinson/BioLingual')\n",
    "\n",
    "#Load audio files\n",
    "# audio_files = ['audio_file']\n",
    "\n",
    "# Generate captions for the recording\n",
    "captions = clap_model.generate_caption(audio_files, resample=True, beam_size=5, entry_length=67, temperature=0.01)\n",
    "\n",
    "# Print the result\n",
    "for i in range(len(audio_files)):\n",
    "    print(f\"Audio file: {audio_files[i]} \\n\")\n",
    "    print(f\"Generated caption: {captions[i]} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a73830d-b32f-45ca-93d6-7b31e9629241",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ClapModel' object has no attribute 'generate_caption'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ClapModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaion/clap-htsat-unfused\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_caption\u001b[49m(audio_files)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ClapModel' object has no attribute 'generate_caption'"
     ]
    }
   ],
   "source": [
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "model.generate_caption(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27451104-f6d4-4ed1-b9cb-08076e6c54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files=audio_files[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d216f35e-d3d6-40ee-bd34-9b61879026b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msclap import CLAP\n",
    "\n",
    "# Load model (Choose version 'clapcap')\n",
    "clap_model = CLAP(version = 'clapcap', use_cuda=False)\n",
    "captions = clap_model.generate_caption(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c8ece0-8b2c-427a-902a-381af9590fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: /storage/UC6_new_data/data/Grafton_15810_train/Grafton_15810_2022-05-08_18-55-33_267-0_Cargo_at-anchor_0-0_2022-05-08-19-00-00_2169.wav \n",
      "\n",
      "Generated caption: A machine is running at a steady pace.  \n",
      "\n",
      "Audio file: /storage/UC6_new_data/data/Grafton_15810_train/Grafton_15810_2022-03-01_20-37-50_130-0_Cargo_at-anchor_0-0_2022-03-01-20-40-00_5666.wav \n",
      "\n",
      "Generated caption: The rain is falling and the wind is blowing.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(audio_files)):\n",
    "    print(f\"Audio file: {audio_files.iloc[i]} \\n\")\n",
    "    print(f\"Generated caption: {captions[i]} \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
